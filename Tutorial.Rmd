---
title: "Correlation and Regression Tutorial"
author: "Amber Kalb"
date: "4/3/2017"
output:
  pdf_document: default
  html_document: default
  word_document: default
---
 
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(repos=structure(c(CRAN="http://cran.r-project.org")))
options(repos="https://cran.rstudio.com" )
install.packages("pscl", repos = "https://cran.rstudio.com")
```
##Introduction

### Statistical Models, what is it good for?
####"All models are wrong, but some are useful"-George Box 
The purpose of statistics is to describe and predict information. This can be divided into descriptive statistics and inferential statistics. Sometimes we collect data in an attempt to describe the characteristics of a population and other times we want to make predictions. It might help to think of statistical modeling as yet another way to evaluate your data, an extension of what we are already doing in class. The biggest difference is, in this tutorial, we will be computationally, rather than visually evaluating our data. 

__Statistical Models__ are built from the data we collect in order to explain a certain phenomenon. Using our collected data, we want to construct a model that represents the processes in real world as close as possible. Once the model is built, we can use it to test our hypotheses and make predictions. 

In other words, models are representations for a purpose. Models will differ depending on your research question (your purpose). 

### Types of Models
We will be going over two types of statistical models that can be built in R: __correlations__ and __linear regressions__.
But, before you can understand the types of statistical models, we must first understand the reason these models exist. Statistical models exist because we are looking for a relationship between two, or sometimes more, variables.

In the case of correlations, we are looking for how much variables co-vary. Correlations can be a quick way to evaluate relationships in an exploratory analysis. 

There are a variety of tests we can run to find the relationship between variables, but the appropriate test will largely depend on the type of variables we include in our model.

####Packages and Dataset 

You will need the following packages installed to perform a basic statistical analysis of your data:
```{r, message=FALSE}
install.packages("pscl", repos = "https://cran.rstudio.com")
install.packages("dplyr")
install.packages("ggplot2")
```

In this tutorial we will be using the diamonds data frame in ggplot2 with the help of the Hmisc library to model the relationship between carat, depth and price. 

Let's look at our at our variables:

```{r, message=FALSE}
library(Hmisc)
library(ggplot2)
```

```{r}
diamonds 
```

As we can see there are continuous and discrete variables. The type of test you run will be dependent on the type of data you want to model. For a correlation, both DV and IV should be continuous. But, for example, if you have two categorical variables, a Chi square test would be appropriate to find out the differences between groups. 

#### Correlations
Let's say you wanted estimate the price of a diamond. You might ask what the relationship is between the carat and the cost of the diamond in order to predict how many ramen noodle packages you might need to consume before you can afford your diamond. Would the relationship be positive between carat and cost (the larger the carat, the higher the cost?), negative (the larger the carat, the lower the cost?), or if there is even a relationship to begin with (the carat size has no relationship with the cost, maybe the cut matters)?

Since our variables (carat and cost) are continuous (interval-ratio), we will be performing a zero-ordered correlation, which will answer two questions: (1) How strongly and (2) in what direction (i.e., +, -) are the IV and DV related? 

```{r, Warning = FALSE, message=FALSE}
attach(diamonds)
plot(carat, price, main="Scatterplot of Price and Carat", 
  	xlab="Carat Size", ylab="Price") 
```

By creating a scatter plot, we are able to see that there is a __positive relationship__ between the carat size and cost of a diamond (a strong pattern in the data). But we do not know the exact strength of that relationship (to what degree they co-vary). Furthermore, carat size does not fully account for the price of the diamond. We can see above that there are smaller diamonds that cost more. What other variables could help explain this variation? 

What can a second variable, such as depth, tell us?  

Let's take a look:

```{r, warning= FALSE}
attach(diamonds)
plot(depth, price, main="Scatterplot of Price and Depth", 
  	xlab="Depth", ylab="price") 
```

In order to definitively check the direction and strength of the relationships, rather than eye-balling it, we would want to run a correlation test.

Let's make sure we meet the assumptions for correlations:
*1. Big N size, check.
*2. Random sampling, check (I think...).
*3. Normal Distribution Curve, let's check. 

```{r, warning= FALSE}

hist(diamonds$price)
```

Remember, a normal distribution takes the shape of a bell-curve. The histogram above certainty doesn't look normally distributed, but let's take a look at one more visual.

```{r}
qqnorm(diamonds$price)
qqline(diamonds$price)
```


If the data was distributed normally, the data points would fall along the line almost perfectly. As we can see above, this is not the case. 

######As a result, you might want to run a non-parametric test, rather than a parametric one (which assumes normality). These can include Kendall's Tau or Spearman's Rho. For more information, please see "Resources" section at the bottom of the tutorial.

Since the variables we are interested in are continuous, we will run Pearson's R which will tell us the strength and direction of the variables' relationship. 

Sig. @: 
  *.05--95% 
  *.01--97% 
	*.001---99% 
	 
```{r}
cor.test(carat, price, method = "pearson")
```

#####Interpretation:

The relationship between the carat size and the price is __statistically significant__, meaning the likelihood of obtaining this result by chance is very small. The t-statistic reinforces this conclusion. 

The strength of Pearsonâ€™s correlation indicates that there is a __strong relationship__ between the variables (perfect, positive relationship at 1, no relationship between the variables at 0, and a perfect negative relationship at -1). The carat size explains approximately __84%__ of the dependent variable: the price of the diamond. In other words, approx. 16% of the variation in price is unexplained by carat size probably due to the influence of other variables (and measurement error, etc.).

Let's see if the depth of a diamond can account for some of this unexplained variance (and thus, creating a fitter model).

```{r}
cor.test(depth, price, method = "pearson")
```

#####Interpretation: 

The relationship between the depth of the diamond and the price is significant.  Pearson's r (cor) tells us that the relationship is weak and negative. In other words, while this relationship is unlikely to be obtained by chance (sig. level), the larger the depth, the diamond is likely to cost less. Furthermore, the depth of a diamond explains less than 1% of the variation in price. 

#### Linear Regression

In simple linear regression a single independent variable is used to predict the value of a dependent variable. Regression provides the line that "best" fits the data as we will see in the scatter plot below. In multiple linear regression two or more independent variables are used to predict the value of a dependent variable. The difference between the two is the number of independent variables included in the model. The type of regression will depend on the type of variables (dichotomous, discrete, continuous, etc.), and the number of IVs. 

If you run a simple linear regression with our x and y variables, carat and cost, then you will get the same results as we have above since the square of Pearson's correlation coefficient is the same as R-squared in a simple linear regression.

__R-squared (Correlation Coefficient)__: Take Pearson's r and square it. Abracadabra, we have calculated our percentage of explained variation (once you move the decimal point two spaces to the right). __Note: R-squared = Explained variation / Total variation__.A value approaching +/- 1.00 indicates a strong linear relationship between variables and thus, less unexplained variation.  

For example, the graph below illustrates a perfect linear relationship. 

```{r}
xyplot(price~price, data = diamonds)
```


Simple (as well as multiple) linear regression is done using the __lm()__ function. For simple regression, the formula takes the form __DV ~ IV__, which should be read something like "DV as modeled by IV" or "DV as predicted by IV". __It's critical to remember that the DV must come before the IV.__ 

```{r}
diamondregcar = lm(price~carat, data = diamonds)
summary(diamondregcar)
```

Now, Let's graph it!

```{r}
plot(price ~ carat, data= diamonds, main="I got Diamonds on Diamonds")
abline(lm(price~carat), col="red")

```

#####Interpretation:
The __simple linear regression output__ gives us much of what we knew from the correlation ran earlier. As mentioned above, the R-squared is Pearson's r, only squared which gives us the proportion of all variation in price that is explained by carat size.   
The scatter plot depicts the relationship between the two variables, with an added bonus: the __regression line__ represents the best-fitted straight line that summarizes the relationship between two variables from our data. 


Let's take a closer look at the regression output for price while controlling for depth. 

```{r}
diamondregdep = lm(price ~ depth, data = diamonds)
summary(diamondregdep)
```

#####Interpretation:
The relationship is significant (as we confirmed earlier), but the depth accounts for less than a percent in the variation of a diamonds price. One might notice that the relationship is not represented as negative in the linear regression (like it had been when we performed a correlation test). That is because we had to square Pearson's r, but when we place a regression line in the scatter plot, the relationship becomes a bit more apparent. 

```{r}
plot(price ~ depth, data= diamonds, main="I Got Diamonds on Diamonds")
abline(lm(price~depth), col="red")
```

When we control for carat size and depth in a multiple regression (see below), we will understand what this means for variation in our model. 


#####Controling for Third Variable
Perhaps the most common and most powerful statistical technique that we use is __Multiple Regression__ where several variables are used collectively to predict scores on a single outcome variable, a quantitative outcome usually. When controlling for multiple variables in a model, this is where statistical analysis is vastly more accurate and easier to interpret the relationship between variables than most visualizations. The formula interface is the same structure for the bi-variate analyses performed above except we will add variables to the model now.

```{r}
multregdiamond = lm(price ~ depth + carat, data = diamonds)
summary(multregdiamond)
```

#####Interpretation:
Similar to our outputs above, the linear regression function gives us a **slightly** higher R-squared of 0.85. While controlling for depth, our variables explain approx. 85% of the variation in the model. 



####Resources 

What statistical test is best for my data? 
[Click here](http://www.socialresearchmethods.net/selstat/p7_2.htm)

[Or here](http://www.csun.edu/~amarenco/Fcs%20682/lecture%206%20statistical%20tests%20decision%20tree.htm)

How do I perform statistical analysis in R? 
[Click here](https://www.lynda.com/R-tutorials.htm)

I want to look at categorical and dichotomous data (perform an ANOVA) not shown in this tutorial. 
[Click here](http://r4ds.had.co.nz/model-intro.html)

I want R equivalents to SAS and SPSS procedures. 
[Click here](http://blog.revolutionanalytics.com/2012/04/r-equivalents-to-sas-and-spss-procedures.html)  

